{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/kriggs/bellabeat-case-study-2-3-feature-selection?scriptVersionId=142870750\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# BellaBeat Case Study 2/3 - Feature Selection\n\nIn this notebook I'll be selecting explanatory features from the Bellabeat Dataset for the modelling of a ML algorith to detect abnormal heart rate conditions.\n\nThe inspiration for this modelling is explained in the 3rd notebook.","metadata":{}},{"cell_type":"markdown","source":"This Study is split into 3 parts:\n- Part 1 - Data exploration and inspiration to the question  \n    https://www.kaggle.com/code/kriggs/bellabeat-case-study-1-3-data-exploration\n\n- Part 2 - Feature selection  \n    This notebook\n\n- Part 3 - ML Modelling  \n    https://www.kaggle.com/kriggs/bellabeat-case-study-3-3-ml-modelling","metadata":{}},{"cell_type":"code","source":"# Imports\n\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom IPython.display import Image\n\nno_top_left_bar = {\"axes.spines.right\": False, \"axes.spines.top\": False}\npd.set_option('display.width', 1000)\npd.set_option('display.max_columns', 500)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-13T12:29:41.126888Z","iopub.execute_input":"2023-09-13T12:29:41.127272Z","iopub.status.idle":"2023-09-13T12:29:43.470893Z","shell.execute_reply.started":"2023-09-13T12:29:41.12724Z","shell.execute_reply":"2023-09-13T12:29:43.469893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading datasets\n\ndata_dir = '/kaggle/input/fitbit/Fitabase Data 4.12.16-5.12.16/'\n\nrelevant_minutes_datasets = [\n    data_dir + 'minuteMETsNarrow_merged.csv',\n    data_dir + 'minuteStepsNarrow_merged.csv',\n    data_dir + 'minuteIntensitiesNarrow_merged.csv',\n    data_dir + 'minuteCaloriesNarrow_merged.csv',\n    data_dir + 'heartrate_seconds_merged.csv'\n]\n\ndfs_minutes = []\n\ndict_rename_date_cols = {\n    'Date': 'Time',\n    'ActivityMinute':'Time',\n    'date': 'Time',\n    'Value':'heart_rate'\n}\n\nfor ds in relevant_minutes_datasets:\n    dfs_minutes.append(pd.read_csv(ds).rename(columns=dict_rename_date_cols))\n\nfor i_df in range(len(dfs_minutes)):\n    dfs_minutes[i_df]['Time'] = pd.to_datetime(dfs_minutes[i_df]['Time'], format=r'%m/%d/%Y %I:%M:%S %p')","metadata":{"execution":{"iopub.status.busy":"2023-09-13T12:29:43.473267Z","iopub.execute_input":"2023-09-13T12:29:43.474253Z","iopub.status.idle":"2023-09-13T12:30:51.245383Z","shell.execute_reply.started":"2023-09-13T12:29:43.474206Z","shell.execute_reply":"2023-09-13T12:30:51.244259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Time bining in 1min frequency\n\nfreq = '1 min'\ndf_concat = pd.concat(dfs_minutes, ignore_index=True)\nfive_min_cut = pd.date_range(start=df_concat['Time'].min(), freq=freq, end=(df_concat['Time'].max() + pd.Timedelta(freq)))\ndf_concat.loc[:, 'time_range'] = pd.cut(df_concat['Time'], five_min_cut, include_lowest=True)\ndf_concat.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-13T12:30:51.246845Z","iopub.execute_input":"2023-09-13T12:30:51.24717Z","iopub.status.idle":"2023-09-13T12:30:53.358466Z","shell.execute_reply.started":"2023-09-13T12:30:51.247143Z","shell.execute_reply":"2023-09-13T12:30:53.357473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Aggregating the bins with the min, max and mean values\n\ndf_grouped = df_concat.groupby(['Id', 'time_range']).agg(['min', 'max', 'mean'])\ndf_grouped = (df_grouped.set_axis(df_grouped.columns.map('_'.join), axis=1).\n              drop(columns=['Time_max', 'Time_mean']).\n              rename(columns={'Time_min':'Time'}))\n\ndf_grouped.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-13T12:30:53.362606Z","iopub.execute_input":"2023-09-13T12:30:53.362981Z","iopub.status.idle":"2023-09-13T12:31:26.011923Z","shell.execute_reply.started":"2023-09-13T12:30:53.362936Z","shell.execute_reply":"2023-09-13T12:31:26.010737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Percentage of each column with NaN values","metadata":{}},{"cell_type":"code","source":"pd.DataFrame(df_grouped.isna().sum() / len(df_grouped)).T.style.format('{:.2%}')","metadata":{"execution":{"iopub.status.busy":"2023-09-13T12:31:26.013294Z","iopub.execute_input":"2023-09-13T12:31:26.013649Z","iopub.status.idle":"2023-09-13T12:31:26.14762Z","shell.execute_reply.started":"2023-09-13T12:31:26.013619Z","shell.execute_reply":"2023-09-13T12:31:26.146503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Removing NaN and setting the name column","metadata":{}},{"cell_type":"code","source":"df_grouped = df_grouped.dropna().reset_index()\ndf_grouped['Name'] = df_grouped['Id'].replace(pd.read_csv('/kaggle/input/bellabeat-case-study-1-3-data-exploration/id_to_names.csv').set_index('Unnamed: 0').to_dict()['Names'])\nprint('Shape: ', df_grouped.shape)\ndf_grouped.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-13T12:31:26.149316Z","iopub.execute_input":"2023-09-13T12:31:26.150062Z","iopub.status.idle":"2023-09-13T12:31:26.342561Z","shell.execute_reply.started":"2023-09-13T12:31:26.150018Z","shell.execute_reply":"2023-09-13T12:31:26.341426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Checking the data correlation","metadata":{}},{"cell_type":"code","source":"def hightlight_1(val):\n    color = 'red' if val == 1 else ''\n    return 'color: %s' % color\n\ndf_grouped._get_numeric_data().drop(columns=['Id']).corr().style.applymap(lambda x: \"background-color: red\" if x > 0.95 else \"\")","metadata":{"execution":{"iopub.status.busy":"2023-09-13T12:27:45.344018Z","iopub.status.idle":"2023-09-13T12:27:45.344401Z","shell.execute_reply.started":"2023-09-13T12:27:45.344203Z","shell.execute_reply":"2023-09-13T12:27:45.344221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; border:#125899 solid; padding: 15px; background-color: #a3bfd9; font-size:100%; text-align:left; color: #000000\"><font color='#125899'>\n\n##  Inference Correlation </font>\n   \n<li> The min/max/mean values are all too correlated to be of any value to be used togheter, only the mean value will be left\n<li> Calories and METs have an extremely high correlation, possibility is that these features are calculated of one another\n</div>\n","metadata":{}},{"cell_type":"code","source":"# Removing all features min/max column\n\ndf_grouped = df_grouped.drop(columns=[\n\t'METs_min', 'METs_max',\n\t'Steps_min', 'Steps_max',\n\t'Intensity_min','Intensity_max',\n\t'Calories_min', 'Calories_max',\n\t'heart_rate_max', 'heart_rate_min'\n]).rename(columns={\n\tc:c.replace('_mean', '') for c in df_grouped.columns\n})\n\ndf_grouped.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-13T12:27:45.345152Z","iopub.status.idle":"2023-09-13T12:27:45.345537Z","shell.execute_reply.started":"2023-09-13T12:27:45.34534Z","shell.execute_reply":"2023-09-13T12:27:45.345357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plotting the correlations","metadata":{}},{"cell_type":"code","source":"sns.pairplot(data=df_grouped._get_numeric_data().drop(columns=['Id']))","metadata":{"execution":{"iopub.status.busy":"2023-09-13T12:27:45.347601Z","iopub.status.idle":"2023-09-13T12:27:45.348583Z","shell.execute_reply.started":"2023-09-13T12:27:45.348269Z","shell.execute_reply":"2023-09-13T12:27:45.348308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(1,3, figsize=(15,4), sharey=True)\naxs = iter(axs.flatten())\n\nfor m in ['pearson', 'spearman', 'kendall']:\n    ax = next(axs)\n    curr_df = df_grouped._get_numeric_data().drop(columns=['Id'])\n    \n    sns.heatmap(curr_df.corr(method=m), annot=True, ax=ax, fmt=\".2f\")\n    ax.set_title(f'Correlation method: {m}')\n    \nfig.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2023-09-13T12:27:45.350827Z","iopub.status.idle":"2023-09-13T12:27:45.351688Z","shell.execute_reply.started":"2023-09-13T12:27:45.351373Z","shell.execute_reply":"2023-09-13T12:27:45.351402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; border:#125899 solid; padding: 15px; background-color: #a3bfd9; font-size:100%; text-align:left; color: #000000\"><font color='#125899'>\n\n##  Inference Feature Correlation </font>\n   \n<li> All the explanatory variables (METs, Steps, Intensity, Calorie) are very high correlated (> 0.7)\n<li> To continue this study, homewever, we'll include 2 explanatory variables, Steps, which has the least correlation with the other explanatory variables, and METs, which is very highly lineary correlated with Calories and intensity\n<li> Since the Spearman's correlation for Steps and Intensity is greater than the Pearson's correlation, the relationship between these two variables are expected to be monotonic but not completely linear\n\n</div>","metadata":{}},{"cell_type":"code","source":"# Removing unnecessary columns\n\ndf_reduced = df_grouped.drop(columns=['Calories', 'Intensity'])\ndf_reduced.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-13T12:27:45.353158Z","iopub.status.idle":"2023-09-13T12:27:45.353712Z","shell.execute_reply.started":"2023-09-13T12:27:45.353421Z","shell.execute_reply":"2023-09-13T12:27:45.353445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check how many values each name has with how many days\n\ndf_count_days_names = df_reduced.groupby([df_reduced.Time.dt.strftime('%m-%d'), 'Name'])['Id'].count()\n\nprint('Number of days each participant logged for more than 10 hours')\ndf_count_days_names[(df_count_days_names > 10 * 60)].reset_index().groupby('Name').Name.count().sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T12:27:45.355474Z","iopub.status.idle":"2023-09-13T12:27:45.356034Z","shell.execute_reply.started":"2023-09-13T12:27:45.355743Z","shell.execute_reply":"2023-09-13T12:27:45.355778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; border:#125899 solid; padding: 15px; background-color: #a3bfd9; font-size:100%; text-align:left; color: #000000\"><font color='#125899'>\n\n##  Inference Days Logged </font>\n   \n<li> A better analysis can be conducted on days that had longer tracking, as such, only days that hat more than 10 hours will be used\n<li> Furthermore, only participants that had more than 10 days logged for more than 10 hours will be used\n\n</div>","metadata":{}},{"cell_type":"code","source":"# Creating DF with aforementioned restrictions\n\ndf_only_more_10_days = df_reduced.set_index([df_reduced.Time.dt.strftime('%m-%d'), 'Name']).loc[df_count_days_names[(df_count_days_names > 10 * 60)].index]\ndf_only_more_10_days.index = df_only_more_10_days.index.rename({'Time':'date'})\ndf_only_more_10_days = df_only_more_10_days.reset_index()\ndf_only_more_10_days.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-13T12:27:45.357337Z","iopub.status.idle":"2023-09-13T12:27:45.357716Z","shell.execute_reply.started":"2023-09-13T12:27:45.357531Z","shell.execute_reply":"2023-09-13T12:27:45.357549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cases selection\n\nThe next part of this study would be to talk to a expert in the field and delimit the days and patients which had a normal heart rate to METs/Steps.\n\nAs this is just a study I will be doing the part of data selection as well, in these next few blocks I will be exporting the data for visualization and selecting manually which data I seem to find fit for a regular HR to METs/Steps.","metadata":{}},{"cell_type":"code","source":"# Saving the graphs in a folder for easier analysis\n!mkdir Graphs_heart_rate_METs_Steps\n\nall_ids = df_only_more_10_days['Id'].unique()\nall_days = df_only_more_10_days['date'].unique()\n\nn_cols = 3\nfig, axs = plt.subplots(int(np.ceil(len(all_days) / n_cols)), n_cols, figsize=(30,40), sharey=True)\naxs = axs.flatten()\ntwinaxs = []\n\nfor id in all_ids:\n\tdf_curr_id = df_only_more_10_days[df_only_more_10_days['Id'] == id]\n\tfor i_day, day in enumerate(all_days[:]):\n\t\tcurr_ax = axs[i_day]\n\t\tdf_curr_id_day = df_curr_id[df_curr_id['date'] == day]\n\t\tscatter_size = 20\n\t\tscatter_alpha = 0.6\n\t\tsns.scatterplot(data=df_curr_id_day, x='heart_rate', y='METs', ax=curr_ax, color='blue', alpha=scatter_alpha, s=scatter_size)\n\t\ttwinaxs.append(sns.scatterplot(data=df_curr_id_day, x='heart_rate', y='Steps', ax=curr_ax.twinx(), color='orange', alpha=scatter_alpha, s=scatter_size))\n\n\t\tcurr_ax.set(\n\t\t\txticks=[],\n\t\t\txlabel='',\n\t\t\ttitle=day,\n\t\t\tylabel='',\n\t\t\tyticks=[]\n\t\t)\n\n\t\ttwinaxs[-1].set(\n\t\t\tylabel='',\n\t\t\tyticks=[]\n\t\t)\n\t\t\n\tfig.suptitle(f'Graphs for {df_curr_id.Name.iloc[0]} - Id: {id}\\n'+\n\t\t\t\t\t'Heart Rate in the X axis\\n'+\n\t\t\t\t\t'METs in blue\\n'+\n\t\t\t\t\t'Steps in orange', fontsize=16)\n\t\n\tfig.tight_layout()\n\tfig.subplots_adjust(top=0.94)\n\tplt.savefig('Graphs_heart_rate_METs_Steps/' + df_curr_id.Name.iloc[0])\n\t\n\tif id != all_ids[-1]:\n\t\tfor ax1, ax2 in zip(axs, twinaxs):\n\t\t\tax1.cla()\n\t\t\tax2.cla()\n\t\t\ttwinaxs = []","metadata":{"execution":{"iopub.status.busy":"2023-09-13T12:27:45.358839Z","iopub.status.idle":"2023-09-13T12:27:45.359866Z","shell.execute_reply.started":"2023-09-13T12:27:45.359561Z","shell.execute_reply":"2023-09-13T12:27:45.359589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Example of selected graphs for training the model","metadata":{}},{"cell_type":"code","source":"Image(filename='/kaggle/input/chosen-graphs/iMarkup_20230822_145446.png') ","metadata":{"execution":{"iopub.status.busy":"2023-09-13T12:27:45.361516Z","iopub.status.idle":"2023-09-13T12:27:45.362064Z","shell.execute_reply.started":"2023-09-13T12:27:45.361787Z","shell.execute_reply":"2023-09-13T12:27:45.361814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plotting chosen Data for training","metadata":{}},{"cell_type":"code","source":"chosen_graphs = {\n\t8877689391:[\n\t\t'04-12', '04-14', '04-19', '04-20', '04-21', '04-22', '04-22', '05-04'\n\t],\n\t2347167796:[\n\t\t'04-16'\n\t],\n\t2022484408:[\n\t\t'04-17', '04-24'\n\t],\n\t6962181067:[\n\t\t'04-21'\n\t],\n\t4388161847:[\n\t\t'04-23', '05-02', '05-08'\n\t]\n}\n\nnum_graphs = 0\nfor _, days in chosen_graphs.items():\n\tnum_graphs += len(days)\n\nn_cols = 3\nfig, axs = plt.subplots(int(np.ceil(num_graphs / n_cols)), n_cols, figsize=(20,20), sharey=True)\naxs = iter(axs.flatten())\n\nfor id, days in chosen_graphs.items():\n\tfor day in days:\n\t\tcurr_ax = next(axs)\n\t\tdf_curr_id_day = df_only_more_10_days[(df_only_more_10_days['date'] == day) & (df_only_more_10_days['Id'] == id)]\n\t\tscatter_size = 20\n\t\tscatter_alpha = 0.6\n\t\tsns.scatterplot(data=df_curr_id_day, x='heart_rate', y='METs', ax=curr_ax, color='blue', alpha=scatter_alpha, s=scatter_size)\n\t\ttwinaxs.append(sns.scatterplot(data=df_curr_id_day, x='heart_rate', y='Steps', ax=curr_ax.twinx(), color='orange', alpha=scatter_alpha, s=scatter_size))\n\n\t\tcurr_ax.set(\n\t\t\txticks=[],\n\t\t\txlabel='',\n\t\t\ttitle=f'Id:{id} on {day}',\n\t\t\tylabel='',\n\t\t\tyticks=[]\n\t\t)\n\n\t\ttwinaxs[-1].set(\n\t\t\tylabel='',\n\t\t\tyticks=[]\n\t\t)\n\t\t\nfig.suptitle(f'Chosen graphs for the training data\\n'+\n\t\t\t\t'Heart Rate in the X axis\\n'+\n\t\t\t\t'METs in blue\\n'+\n\t\t\t\t'Steps in orange', fontsize=16)\n\nfig.tight_layout()\nfig.subplots_adjust(top=0.90)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T12:27:45.363653Z","iopub.status.idle":"2023-09-13T12:27:45.364194Z","shell.execute_reply.started":"2023-09-13T12:27:45.363912Z","shell.execute_reply":"2023-09-13T12:27:45.363939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating the Dataset for training and saving in a pickle file","metadata":{}},{"cell_type":"code","source":"df_only_more_10_days['train'] = False\n\nfor id, days in chosen_graphs.items():\n\tfor day in days:\n\t\tdf_only_more_10_days.loc[(df_only_more_10_days['date'] == day) & (df_only_more_10_days['Id'] == id), 'train'] = True\n\nprint('Number of datapoints for the train dataset: ', df_only_more_10_days['train'].sum())\nprint('Total datapoints on the dataset: ', len(df_only_more_10_days))\ndf_only_more_10_days.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-13T12:27:45.365968Z","iopub.status.idle":"2023-09-13T12:27:45.366524Z","shell.execute_reply.started":"2023-09-13T12:27:45.366222Z","shell.execute_reply":"2023-09-13T12:27:45.366247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_only_more_10_days.to_pickle('reduced_dataset_with_train_data.pickle')","metadata":{"execution":{"iopub.status.busy":"2023-09-13T12:27:45.367905Z","iopub.status.idle":"2023-09-13T12:27:45.368267Z","shell.execute_reply.started":"2023-09-13T12:27:45.368086Z","shell.execute_reply":"2023-09-13T12:27:45.368103Z"},"trusted":true},"execution_count":null,"outputs":[]}],"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}}