{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/kriggs/bellabeat-case-study-3-3-ml-modelling?scriptVersionId=142865244\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# BellaBeat Case Study 3/3 - ML Modelling\n\nIn this notebook I'll be creating ML models to predict abnormal heart rate conditions based off the Dataset from Fitbit in the BellaBeat case study.\n\nThe inspiration for the ML algorithm is based off the video \"Anomaly detection with TensorFlow\" by Laurence Moroney.\n\nThe very short version of the video is, if you train a good ML model on \"normal\" data, then there will be huge residuals when predicting on \"abnormal\" data.\n\nThe \"normal\" data was selected in part 2 of this study.","metadata":{}},{"cell_type":"markdown","source":"This Study is split into 3 parts:\n- Part 1 - Data exploration and inspiration to the question  \n    https://www.kaggle.com/code/kriggs/bellabeat-case-study-1-3-data-exploration\n\n- Part 2 - Feature selection  \n    https://www.kaggle.com/code/kriggs/bellabeat-case-study-2-3-feature-selection\n\n- Part 3 - ML Modelling  \n    This notebook","metadata":{}},{"cell_type":"markdown","source":"## Intro: Imports, data loading and training data visualization","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport numpy as np\n\nfrom matplotlib import pyplot as plt\nfrom sklearn.ensemble import IsolationForest\n\nimport matplotlib as mpl\nmpl.rcParams['lines.markersize'] = 4\n\nRED = '#fe6760'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-12T19:57:01.553868Z","iopub.execute_input":"2023-09-12T19:57:01.555211Z","iopub.status.idle":"2023-09-12T19:57:01.561037Z","shell.execute_reply.started":"2023-09-12T19:57:01.555169Z","shell.execute_reply":"2023-09-12T19:57:01.560023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_pickle('/kaggle/input/reduced-dataset-with-train-datapickle/reduced_dataset_with_train_data.pickle')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T19:57:01.564151Z","iopub.execute_input":"2023-09-12T19:57:01.56522Z","iopub.status.idle":"2023-09-12T19:57:01.634643Z","shell.execute_reply.started":"2023-09-12T19:57:01.565188Z","shell.execute_reply":"2023-09-12T19:57:01.63348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# A look into the train data\n\nfeature_cols = ['METs', 'Steps', 'heart_rate']\n\ndf.loc[df['train'] == True, feature_cols]","metadata":{"execution":{"iopub.status.busy":"2023-09-12T19:57:01.636688Z","iopub.execute_input":"2023-09-12T19:57:01.637049Z","iopub.status.idle":"2023-09-12T19:57:01.658073Z","shell.execute_reply.started":"2023-09-12T19:57:01.637018Z","shell.execute_reply":"2023-09-12T19:57:01.656832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting the train data\n\nX = df[df['train'] == True][feature_cols]\n\nax = sns.scatterplot(data=X, x='heart_rate', y='METs', s=10, alpha=0.4)\nax2 = sns.scatterplot(data=X, x='heart_rate', y='Steps', ax=ax.twinx(), color='green', s=10, alpha=0.2);\n\nax.set(\n    title='METs and Steps (2nd axis) plotted on Heart Rate\\nTraining Dataset',\n    xlabel='Heart rate',\n    ylabel='METs (Blue)'\n)\n\nax2.set(\n    ylabel='Steps (green)'\n);","metadata":{"execution":{"iopub.status.busy":"2023-09-12T19:57:01.659307Z","iopub.execute_input":"2023-09-12T19:57:01.659619Z","iopub.status.idle":"2023-09-12T19:57:02.389446Z","shell.execute_reply.started":"2023-09-12T19:57:01.659591Z","shell.execute_reply":"2023-09-12T19:57:02.388135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Isolation Forest\n\nIsolation forest is an anomaly detection algorithm, where the forest goes all the way to a single leaf, and the shortest paths to isolating a point are considered outliers.","metadata":{}},{"cell_type":"code","source":"clf = IsolationForest(n_jobs=-1, contamination=0.1).fit(X.to_numpy())\nX_with_outliers = X.copy()\nX_with_outliers['outliers_iso'] = clf.predict(X.to_numpy())\nX_with_outliers['outliers_iso'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T19:57:02.393059Z","iopub.execute_input":"2023-09-12T19:57:02.393881Z","iopub.status.idle":"2023-09-12T19:57:03.475039Z","shell.execute_reply.started":"2023-09-12T19:57:02.393836Z","shell.execute_reply":"2023-09-12T19:57:03.473575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(1,2, figsize=(10,5), sharey=False)\naxs = axs.flatten()\ntwinaxs = []\n\nfig.suptitle(\n\t'Isolation Forest Outliers (in red)\\nTraining Dataset'\n)\n\nsets_1st_plot = {\n\t'title':'METs vs Heart rate'\n}\n\nsets_2nd_plot = {\n\t'title':'Steps vs Heart rate'\n}\n\n(sns.scatterplot(data=X_with_outliers.query('outliers_iso == -1'), x='heart_rate', y='METs', color='red', s=10, alpha=0.5, ax=axs[0]).set(**sets_1st_plot))\ntwinaxs.append(sns.scatterplot(data=X_with_outliers.query('outliers_iso == 1'), x='heart_rate', y='METs', s=10, alpha=0.8, ax=axs[0].twinx()))\n\nsns.scatterplot(data=X_with_outliers.query('outliers_iso == -1'), x='heart_rate', y='Steps', color='red', s=10, alpha=0.5, ax=axs[1]).set(**sets_2nd_plot);\ntwinaxs.append(sns.scatterplot(data=X_with_outliers.query('outliers_iso == 1'), x='heart_rate', y='Steps', s=10, alpha=0.5, ax=axs[1].twinx(), color='green'))\n\nfor i_ax, i_twinax in zip(axs, twinaxs):\n\tymin = np.min([*i_ax.get_ylim(), *i_twinax.get_ylim()])\n\tymax = np.max([*i_ax.get_ylim(), *i_twinax.get_ylim()])\n\n\ti_ax.set(\n\t\txlabel='Heart Rate'\n\t)\n\n\ti_twinax.set(\n\t\tyticks=[],\n\t\tylabel=''\n\t)\n\n\tfor axs in [i_ax, i_twinax]:\n\t\taxs.set(\n\t\t\tylim=[ymin, ymax]\n\t\t)\n\nfig.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T19:57:03.476425Z","iopub.execute_input":"2023-09-12T19:57:03.476765Z","iopub.status.idle":"2023-09-12T19:57:04.804693Z","shell.execute_reply.started":"2023-09-12T19:57:03.476734Z","shell.execute_reply":"2023-09-12T19:57:04.8033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['outliers_iso'] = clf.predict(df[feature_cols])\n\nfig, axs = plt.subplots(1,2, figsize=(10,5), sharey=False)\naxs = axs.flatten()\ntwinaxs = []\n\nfig.suptitle(\n\t'Isolation Forest Outliers (in red)\\nWhole Dataset'\n)\n\nsets_1st_plot = {\n\t'title':'METs vs Heart rate'\n}\n\nsets_2nd_plot = {\n\t'title':'Steps vs Heart rate'\n}\n\n(sns.scatterplot(data=df.query('outliers_iso == -1'), x='heart_rate', y='METs', color='red', s=10, alpha=0.5, ax=axs[0]).set(**sets_1st_plot))\ntwinaxs.append(sns.scatterplot(data=X_with_outliers.query('outliers_iso == 1'), x='heart_rate', y='METs', s=10, alpha=0.8, ax=axs[0].twinx()))\n\nsns.scatterplot(data=df.query('outliers_iso == -1'), x='heart_rate', y='Steps', color='red', s=10, alpha=0.5, ax=axs[1]).set(**sets_2nd_plot);\ntwinaxs.append(sns.scatterplot(data=X_with_outliers.query('outliers_iso == 1'), x='heart_rate', y='Steps', s=10, alpha=0.5, ax=axs[1].twinx(), color='green'))\n\nfor i_ax, i_twinax in zip(axs, twinaxs):\n\tymin = np.min([*i_ax.get_ylim(), *i_twinax.get_ylim()])\n\tymax = np.max([*i_ax.get_ylim(), *i_twinax.get_ylim()])\n\n\ti_ax.set(\n\t\txlabel='Heart Rate'\n\t)\n\n\ti_twinax.set(\n\t\tyticks=[],\n\t\tylabel=''\n\t)\n\n\tfor axs in [i_ax, i_twinax]:\n\t\taxs.set(\n\t\t\tylim=[ymin, ymax]\n\t\t)\n\nfig.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T19:57:04.806534Z","iopub.execute_input":"2023-09-12T19:57:04.80697Z","iopub.status.idle":"2023-09-12T19:57:12.936151Z","shell.execute_reply.started":"2023-09-12T19:57:04.806929Z","shell.execute_reply":"2023-09-12T19:57:12.934981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; border:#125899 solid; padding: 15px; background-color: #a3bfd9; font-size:100%; text-align:left; color: #000000\"><font color='#125899'>\n\n##  Conclusion Isolation Forest </font>\n   \n<li> Isolation forest does have it uses in the industry, for example fraud detection, unfortunately for this use case it just detected as any high value to be an anomaly\n<li> Hyperparameter tunning was performed, but didn't yield much difference.\n\n</div>","metadata":{}},{"cell_type":"markdown","source":"# 2. Linear Regression\n\nHere I tested 3 linear regression model, the simple one without any hyperparameters, and Ridge and Lasso with regularization parameters to avoid overfitting.\n\nThey were all tested and the one with the lowest R2 score was selected.","metadata":{}},{"cell_type":"code","source":"y_train = X['heart_rate'].copy()\nX_train = X.drop(columns='heart_rate')","metadata":{"execution":{"iopub.status.busy":"2023-09-12T19:57:12.938218Z","iopub.execute_input":"2023-09-12T19:57:12.939018Z","iopub.status.idle":"2023-09-12T19:57:12.947844Z","shell.execute_reply.started":"2023-09-12T19:57:12.938976Z","shell.execute_reply":"2023-09-12T19:57:12.946462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def draw_1_1_line(ax:plt.axes) -> None:\n\t_, xmax, _, ymax = ax.axis()\n\taxis_limit = [0, min(xmax, ymax)]\n\tsns.lineplot(x=axis_limit, y=axis_limit, color='r', ax=ax)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T19:57:12.94999Z","iopub.execute_input":"2023-09-12T19:57:12.950427Z","iopub.status.idle":"2023-09-12T19:57:12.958318Z","shell.execute_reply.started":"2023-09-12T19:57:12.950386Z","shell.execute_reply":"2023-09-12T19:57:12.957241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression, Ridge, Lasso\n\n# Class to hold the best linear model\nclass class_best_regressor:\n\tscore = 0\n\tmodel = LinearRegression().fit(X_train, y_train)\n\n\tdef check_regressor(self, regressor):\n\t\tnew_score = regressor.score(X_train, y_train)\n\t\tif(new_score > self.score):\n\t\t\tself.score = new_score\n\t\t\tself.model = regressor\n\n\tdef get_name(self) -> str:\n\t\treturn self.model.__class__.__name__\n\nbest_regressor = class_best_regressor()\n\n# Simple linear regression\nlinreg = LinearRegression().fit(X_train, y_train)\n\nbest_regressor.check_regressor(linreg)\n\n# Lasso model\n\nfor alpha in [0.00001, 0.001, 0.005, 0.05, 0.5, 1]:\n\tlinlasso = Lasso(alpha=alpha, max_iter = 100_000).fit(X_train, y_train)\n\tbest_regressor.check_regressor(linlasso)\n\n# Ridge Model\n\nfor alpha in [0.00001, 0.001, 0.005, 0.05, 0.5, 1]:\n\tlinridge = Ridge(alpha=alpha, max_iter = 10000).fit(X_train, y_train)\n\tbest_regressor.check_regressor(linridge)\n\n# Print best linear model\n\nprint(f'Best regressor: {best_regressor.model}')\n\n# Plot best model\n\nfig, axs =  plt.subplots(2,2, figsize=(10,10))\nfig.suptitle(f'Linear Regression - Prediction for Heart Rate\\nBest model: {best_regressor.get_name()}')\naxs = iter(axs.reshape(1, -1)[0])\n\n# Plotting of the train dataset\nax = next(axs)\ny_hat_train = pd.Series(best_regressor.model.predict(X_train), index=y_train.index).sort_values()\nsns.scatterplot(x=y_hat_train, y=y_train, marker= 'o', s=20, alpha=0.8, ax=ax)\nax.set(ylabel='Real value',\n       xlabel='Predicted value',\n\t   title=f'Real vs Predicted value\\nTrain Dataset - R²={best_regressor.model.score(X_train, y_train):.3f}'\n)\n\ndraw_1_1_line(ax)\n\n# Plotting whole dataset\nX_whole = df[X_train.columns]\nax = next(axs)\ny_hat = pd.Series(best_regressor.model.predict(X_whole), index=X_whole.index).sort_values()\nsns.scatterplot(x=y_hat, y=df['heart_rate'].loc[y_hat.index], marker= 'o', s=20, alpha=0.8, ax=ax)\nax.set(ylabel='Real value',\n       xlabel='Predicted value',\n\t   title=f'Real vs Predicted value\\nWhole Dataset'\n)\n\ndraw_1_1_line(ax)\n\n# Plotting of residuals Scatterplot\n\nax = next(axs)\nsorted_y_train = y_train.sort_values()\nsorted_x_train = X_train.reindex(sorted_y_train.index)\npredicted_values = best_regressor.model.predict(sorted_x_train)\nresiduals = sorted_y_train - predicted_values\nsns.scatterplot(x=predicted_values, y=residuals, ax=ax)\nax.set(ylabel='Residuals',\n       xlabel='Predicted value',\n\t   title=f'Residuals for the train Dataset'\n);\n\n# Plotting of residuals Histogram\n\nax = next(axs)\nsns.histplot(data=residuals, ax=ax, kde=True)\nax.set(ylabel='Residual count',\n       xlabel='Predicted value',\n\t   title=f'Residuals histogram for the train Dataset'\n);\n\nfig.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T19:58:30.591048Z","iopub.execute_input":"2023-09-12T19:58:30.591577Z","iopub.status.idle":"2023-09-12T19:58:34.288607Z","shell.execute_reply.started":"2023-09-12T19:58:30.591541Z","shell.execute_reply":"2023-09-12T19:58:34.287451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; border:#125899 solid; padding: 15px; background-color: #a3bfd9; font-size:100%; text-align:left; color: #000000\"><font color='#125899'>\n\n##  Inference Residuals Linear Regression </font>\n   \n<li> On the 1st and 2nd graph we see the real value plotted on the y axis against the predicted value from the best model in the X axis, in a perfect model all the points would fall on the red line\n<li> The residuals \"shape\" on the top graphs (train dataset and whole dataset, respectively) show that the train selection was a good representation of the whole dataset\n<li> The points that deviate too much from this red line, as in, have greater residuals, will be considered outliers\n\n</div>","metadata":{}},{"cell_type":"markdown","source":"For the definition of what we'll consider an outlier, we'll use the rule of 3x the standard deviation from the mean.","metadata":{}},{"cell_type":"code","source":"def get_outliers(series: pd.Series, by:str='zscore') -> tuple[float, float]:\n\t\"\"\"\n    Get the lower and upper bounds of the outliers in a pandas series.\n\n    Parameters:\n    -----------\n    series : pd.Series\n        The pandas series to get the outliers from.\n    by : str, optional (default='zscore')\n        The method to use for calculating the outliers. Can be 'IQR' or 'zscore'.\n\n    Returns:\n    --------\n    tuple(float, float)\n        A tuple containing the lower and upper bounds of the outliers.\n    \"\"\"\n\tif by == 'IQR':\n\t\tq25 = series.quantile(.25)\n\t\tq75 = series.quantile(.75)\n\t\tiqr = q75 - q25\n\n\t\treturn q25 - (1.5 * iqr), q75 + (1.5 * iqr)\n\n\tif by == 'zscore':\n\t\tmean, std = series.agg(['mean', 'std'])\n\t\treturn mean - 3 * std, mean + 3 * std\n\t\n\traise ValueError(f'\"by\" not found: {by}')","metadata":{"execution":{"iopub.status.busy":"2023-09-12T19:58:34.291059Z","iopub.execute_input":"2023-09-12T19:58:34.292097Z","iopub.status.idle":"2023-09-12T19:58:34.30883Z","shell.execute_reply.started":"2023-09-12T19:58:34.292055Z","shell.execute_reply":"2023-09-12T19:58:34.305968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lower_bound, upper_bound = get_outliers(pd.Series(residuals), by='zscore')\ndf_linear = df.copy()\ndf_linear['predicted'] = best_regressor.model.predict(df_linear[['METs', 'Steps']])\ndf_linear['residuals'] = df_linear['heart_rate'] - df_linear['predicted']\ndf_linear['outliers_lin'] = np.where((df_linear['residuals'] < lower_bound) | (df_linear['residuals'] > upper_bound), True, False)\ndf_linear.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T19:58:49.799447Z","iopub.execute_input":"2023-09-12T19:58:49.799887Z","iopub.status.idle":"2023-09-12T19:58:49.879015Z","shell.execute_reply.started":"2023-09-12T19:58:49.799857Z","shell.execute_reply":"2023-09-12T19:58:49.877793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(1,2, figsize=(10,5))\naxs = axs.flatten()\n\nfig.suptitle('Outliers (in red) for the linear regression model')\n\nax = axs[0]\nsns.scatterplot(data=df_linear, x='heart_rate', y='METs', ax=ax)\nsns.scatterplot(data=df_linear[df_linear['outliers_lin'] == True], x='heart_rate', y='METs', color=RED, ax=ax)\n\nx_lim, y_lim = (ax.get_xlim(), ax.get_ylim())\n\ny_coefs = np.array(ax.get_ylim())\nx_coefs = y_coefs * best_regressor.model.coef_[0] + (lower_bound + best_regressor.model.intercept_)\nx_coefs2 = y_coefs * best_regressor.model.coef_[0] + (upper_bound + best_regressor.model.intercept_)\n\nsns.lineplot(x=x_coefs, y=y_coefs, ax=ax, color=RED, linestyle='--')\nsns.lineplot(x=x_coefs2, y=y_coefs, ax=ax, color=RED, linestyle='--')\n\ntext_args = {'rotation':48, 'fontweight':400, 'ha':'center', 'size':10}\nax.text(123,90, 'Low HR for high MET\\nThreshold', **text_args)\nax.text(190,69, 'High HR for low MET\\nThreshold', **text_args)\n\nax.set(xlim=x_lim, ylim=y_lim)\n\nax = axs[1]\nsns.scatterplot(data=df_linear, x='heart_rate', y='Steps', ax=ax, color='green')\nsns.scatterplot(data=df_linear[df_linear['outliers_lin'] == True], x='heart_rate', y='Steps', color=RED, ax=ax)\n\nfor ax in axs:\n\tax.set(\n\t\txlabel='Heart Rate'\n\t)\n\nfig.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T19:59:59.746643Z","iopub.execute_input":"2023-09-12T19:59:59.747085Z","iopub.status.idle":"2023-09-12T20:00:03.001797Z","shell.execute_reply.started":"2023-09-12T19:59:59.747049Z","shell.execute_reply":"2023-09-12T20:00:02.999932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(best_regressor.model.coef_.reshape(1,2), columns=['Coef for METs', 'Coef for Steps'])","metadata":{"execution":{"iopub.status.busy":"2023-09-12T19:58:52.584209Z","iopub.execute_input":"2023-09-12T19:58:52.584687Z","iopub.status.idle":"2023-09-12T19:58:52.598256Z","shell.execute_reply.started":"2023-09-12T19:58:52.584645Z","shell.execute_reply":"2023-09-12T19:58:52.59707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; border:#125899 solid; padding: 15px; background-color: #a3bfd9; font-size:100%; text-align:left; color: #000000\"><font color='#125899'>\n\n##  Inference Linear Regression </font>\n   \n<li> In the above graphs and the coeficients we can see that the linear regression considered an almost perfect linear relationship between HR and METs, this was also corroborated by the correlation variables\n<li> The red dashed lines in the graph on the left are the threshold for what the model considers an outlier based solely on METs, the non-outlier points in the \"Low HR for high MET\" region is explained by the negative relationship from Steps, the same is valid for the outliers inside the non-outlier region\n\n</div>","metadata":{}},{"cell_type":"markdown","source":"## Visualizing these outliers during the day","metadata":{}},{"cell_type":"code","source":"print('Outliers detected in the Linear Regression: ', df_linear['outliers_lin'].sum())\nprint('Dates with most outliers per user:')\noutliers_per_day_user = (df_linear[df_linear['outliers_lin'] == True]\n\t\t\t\t\t\t .groupby(['date', 'Name'])['Id']\n\t\t\t\t\t\t .count()\n\t\t\t\t\t\t .unstack()\n\t\t\t\t\t\t .reset_index()\n\t\t\t\t\t\t .melt(id_vars=['date'])\n\t\t\t\t\t\t .sort_values('value', ascending=False))\n\noutliers_per_day_user.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T20:02:04.497073Z","iopub.execute_input":"2023-09-12T20:02:04.497653Z","iopub.status.idle":"2023-09-12T20:02:04.530289Z","shell.execute_reply.started":"2023-09-12T20:02:04.49762Z","shell.execute_reply":"2023-09-12T20:02:04.52913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_cols = 2\nnum_rows = 5\n\nsns.set_theme(style=\"ticks\")\nfig, axs = plt.subplots(num_rows, num_cols, figsize=(15,20), sharex=False, sharey=True)\nfig.suptitle('Days withs most outliers according to the linear model', y=0.99)\naxs = iter(axs.flatten())\n\nfor date, name in outliers_per_day_user[['date', 'Name']].head(10).values:\n\tax = next(axs)\n\tmask_date = df_linear['date'] == date\n\tmask_name = df_linear['Name'] == name\n\tsns.scatterplot(data=df_linear[mask_date & mask_name], x='METs', y='heart_rate', ax=ax, hue='outliers_lin', s=12)\n\tax.set(\n\t\txlabel='METs',\n\t\txticks=[],\n\t\tylabel='Heart rate',\n\t\ttitle=f'{name} on {date}'\n\t)\n\t\n\th, _ = ax.get_legend_handles_labels()\n\tax.legend(\n\t\ttitle='',\n\t\thandles=h,\n\t\tlabels=['Not an outlier', 'Outlier']\n\t)\n\nfig.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T20:15:44.155288Z","iopub.execute_input":"2023-09-12T20:15:44.155886Z","iopub.status.idle":"2023-09-12T20:15:49.512156Z","shell.execute_reply.started":"2023-09-12T20:15:44.155825Z","shell.execute_reply":"2023-09-12T20:15:49.511141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; border:#125899 solid; padding: 15px; background-color: #72d798; font-size:100%; text-align:left; color: #000000\"><font color='#1a3b1c'>\n\n##  Conclusions </font>\n\nIn this study I explored and analysed the BellaBeat dataset statiscally and built a model for detecting heart rate anomalies based on measurements from the tracking devices, the feature of abnormal HR could be integrated in the tracking devices OTA without the need for extra costs, improving the features of the devices using only data.\n    \nIn the first part of this study questioned \"is it possible to automate the anomaly detection of the HR\", and to this question I showed here that with this data and method: maybe.\n    \nIn the last graphs of this study the models did selected a high HR for a low MET, but the model were extremely simple so this is to be considered a proof of concept, here are some suggestions that could improve this model and concept:\n    \n<li> Access to more user data, during the exploration weight data was very scarce and could not be used, age, sex and many others, which understandably couldn't be shared, could also improve a lot the model\n<li> Cross Validation was not performed and would definitely improve the model\n<li> Feature engineering, as the exploration showed non-linear relationships\n<li> Feature scaling on the models\n<li> Test other models when more data is available - I ran TensorFlow but it overfitted very easily, also it was harder to explain graphically as to what's an outlier with boundaries\n\n</div>\n","metadata":{}}],"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}}